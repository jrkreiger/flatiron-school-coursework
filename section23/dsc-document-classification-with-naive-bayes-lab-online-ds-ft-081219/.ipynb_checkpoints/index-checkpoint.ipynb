{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Classification with Naive Bayes - Lab\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this lecture, you'll practice implementing the Naive Bayes algorithm on your own.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "You will be able to:  \n",
    "\n",
    "* Implement document classification using Naive Bayes\n",
    "* Understand the need for the Laplacian smoothing correction\n",
    "* Explain how to code a bag of words representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the Dataset\n",
    "\n",
    "To start, import the dataset stored in the text file `SMSSpamCollection`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T19:50:03.362687Z",
     "start_time": "2019-10-02T19:50:02.783349Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T19:58:09.870716Z",
     "start_time": "2019-10-02T19:58:09.843674Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('SMSSpamCollection', header=None, sep='\\t')\n",
    "df.columns = ['label', 'text']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T19:58:13.439911Z",
     "start_time": "2019-10-02T19:58:13.427158Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     4825\n",
       "spam     747\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Account for Class Imbalance\n",
    "\n",
    "To help your algorithm perform more accurately, subset the dataset so that the two classes are of equal size. To do this, keep all of the instances of the minority class (spam) and subset examples of the majority class (ham) to an equal number of examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T19:58:21.426984Z",
     "start_time": "2019-10-02T19:58:21.411508Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "747"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Your code here\n",
    "spam = df[df['label'] == 'spam']\n",
    "num_spam = len(spam)\n",
    "ham = df[df['label'] == 'ham'][:num_spam]\n",
    "len(ham)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T19:58:23.831886Z",
     "start_time": "2019-10-02T19:58:23.819743Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     747\n",
       "spam    747\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.concat([spam, ham])\n",
    "df2['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train - Test Split\n",
    "\n",
    "Now implement a train test split on your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T20:00:55.457915Z",
     "start_time": "2019-10-02T20:00:55.041352Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df2['text']\n",
    "y = df2['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=17)\n",
    "train_df = pd.concat([X_train, y_train], axis=1)\n",
    "test_df = pd.concat([X_test, y_test], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Word Frequency Dictionary for Each Class\n",
    "\n",
    "Create a word frequency dictionary for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T20:02:56.573039Z",
     "start_time": "2019-10-02T20:02:56.523420Z"
    }
   },
   "outputs": [],
   "source": [
    "class_word_freq = {}\n",
    "classes = train_df.label.unique()\n",
    "\n",
    "for class_ in classes:\n",
    "    temp = train_df[train_df.label == class_]\n",
    "    bag = {}\n",
    "    for row in temp.index:\n",
    "        doc = temp['text'][row]\n",
    "        for word in doc.split():\n",
    "            bag[word] = bag.get(word, 0) + 1\n",
    "    class_word_freq[class_] = bag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count the Total Corpus Words\n",
    "Calculate V, the total number of words in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T20:04:30.064922Z",
     "start_time": "2019-10-02T20:04:30.046374Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5926"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = set()\n",
    "for text in train_df.text:\n",
    "    for word in text.split():\n",
    "        vocab.add(word)\n",
    "V = len(vocab)\n",
    "V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Bag of Words Function\n",
    "\n",
    "Before implementing the entire Naive Bayes algorithm, create a helper function `bag_it()` to create a bag of words representation from a document's text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T20:08:11.193959Z",
     "start_time": "2019-10-02T20:08:11.188861Z"
    }
   },
   "outputs": [],
   "source": [
    "def bag_it(doc):\n",
    "    bag = {}\n",
    "    for word in doc.split():\n",
    "        bag[word] = bag.get(word, 0) + 1\n",
    "    return bag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Naive Bayes\n",
    "\n",
    "Now, implement a master function to build a naive Bayes classifier. Be sure to use the logarithmic probabilities to avoid underflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T20:10:46.323601Z",
     "start_time": "2019-10-02T20:10:46.313963Z"
    }
   },
   "outputs": [],
   "source": [
    "def nb_classify(doc, class_word_freq, V, return_posteriors=False):\n",
    "    bag = bag_it(doc)\n",
    "    classes = []\n",
    "    posteriors = []\n",
    "    for class_ in class_word_freq.keys():\n",
    "        p = np.log(0.5)\n",
    "        for word in bag.keys():\n",
    "            num = bag[word] + 1\n",
    "            denom = class_word_freq[class_].get(word, 0) + V\n",
    "            p += np.log(num/denom)\n",
    "        classes.append(class_)\n",
    "        posteriors.append(p)\n",
    "    if return_posteriors:\n",
    "        print(posteriors)\n",
    "    return classes[np.argmax(posteriors)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Out Your Classifier\n",
    "\n",
    "Finally, test out your classifier and measure its accuracy. Don't be perturbed if your results are sub-par; industry use cases would require substantial additional preprocessing before implementing the algorithm in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T20:11:44.757968Z",
     "start_time": "2019-10-02T20:11:44.620312Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    0.761607\n",
       "True     0.238393\n",
       "dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat_train = X_train.map(lambda x: nb_classify(x, class_word_freq, V))\n",
    "residuals = y_train == y_hat_train\n",
    "residuals.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level-Up\n",
    "\n",
    "Rework your code into an appropriate class structure so that you could easily implement the algorithm on any given dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T20:33:15.750983Z",
     "start_time": "2019-10-02T20:33:15.738225Z"
    }
   },
   "outputs": [],
   "source": [
    "class NaiveBayesDocClassifier:\n",
    "    \"\"\"Perform Naive Bayes classification of texts.\n",
    "       Takes a DataFrame containing 'label' and 'text' columns\n",
    "       and provides methods for predicting labels for the texts.\"\"\"\n",
    "    \n",
    "    # Define a function to make a bag of words from a string\n",
    "    def bag_it(doc):\n",
    "        bag = {}\n",
    "        for word in doc.split():\n",
    "            bag[word] = bag.get(word, 0) + 1\n",
    "        return bag\n",
    "\n",
    "    # Create a dictionary with classes (labels) as keys and \n",
    "    # bags-of-words for those classes as values\n",
    "    def get_class_word_freq(self, df):\n",
    "        class_word_freq = {}\n",
    "        classes = df.label.unique()\n",
    "\n",
    "        for class_ in classes:\n",
    "            temp = df[df.label == class_]\n",
    "            bag = {}\n",
    "            for row in temp.index:\n",
    "                doc = temp['text'][row]\n",
    "                bag = bag_it(doc)\n",
    "            class_word_freq[class_] = bag\n",
    "        \n",
    "        return class_word_freq\n",
    "    \n",
    "    # Define a function to find the number of unique words in a corpus\n",
    "    def count_vocab(self, df):\n",
    "        vocab = set()\n",
    "        for text in df.text:\n",
    "            for word in text.split():\n",
    "                vocab.add(word)\n",
    "        return len(vocab)\n",
    "    \n",
    "    # Define a function to classify a single document\n",
    "    def nbay_classify(self, df, doc, return_posteriors=False):\n",
    "        bag = bag_it(df['text'][doc])\n",
    "        class_word_freq = self.get_class_word_freq(df)\n",
    "        V = self.count_vocab(df)\n",
    "        classes = []\n",
    "        posteriors = []\n",
    "        for class_ in class_word_freq.keys():\n",
    "            p = np.log(0.5)\n",
    "            for word in bag.keys():\n",
    "                num = bag[word] + 1\n",
    "                denom = class_word_freq[class_].get(word, 0) + V\n",
    "                p += np.log(num/denom)\n",
    "            classes.append(class_)\n",
    "            posteriors.append(p)\n",
    "        if return_posteriors:\n",
    "            print(posteriors)\n",
    "        return classes[np.argmax(posteriors)]\n",
    "    \n",
    "    # Define a function to run the whole naive Bayes algorithm\n",
    "    def run_nb_algorithm(self, df):\n",
    "        y_hat = []\n",
    "        for row in df.index:\n",
    "            pred = self.nbay_classify(df, row)\n",
    "            y_hat.append(pred)\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T20:33:16.120442Z",
     "start_time": "2019-10-02T20:33:16.114720Z"
    }
   },
   "outputs": [],
   "source": [
    "classy = NaiveBayesDocClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T20:38:26.411392Z",
     "start_time": "2019-10-02T20:37:50.995994Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'ham': 1109, 'spam': 11})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "y_hat = classy.run_nb_algorithm(train_df)\n",
    "Counter(y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Well done! In this lab, you practiced implementing Naive Bayes for document classification!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
